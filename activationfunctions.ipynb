{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions in Neural Networks\n",
    "\n",
    "Activation functions introduce non-linearity into the network, enabling it to learn and model complex patterns. Below is an overview of common activation functions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Sigmoid Activation**\n",
    "- **Equation**: \n",
    "  $$\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "\n",
    "### Detailed Explanation:\n",
    "1. **Function Behavior**:\n",
    "   - As x approaches positive infinity, σ(x) approaches 1\n",
    "   - As x approaches negative infinity, σ(x) approaches 0\n",
    "   - At x = 0, σ(x) = 0.5\n",
    "\n",
    "2. **S-shaped Curve**:\n",
    "   - The sigmoid function produces an S-shaped curve\n",
    "   - This shape allows for smooth transitions between 0 and 1\n",
    "\n",
    "3. **Derivative**:\n",
    "   - The derivative of the sigmoid function is:\n",
    "     $$\n",
    "     \\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))\n",
    "     $$\n",
    "   - This property makes it computationally efficient for backpropagation\n",
    "\n",
    "4. **Symmetry**:\n",
    "   - The sigmoid function is symmetric around the point (0, 0.5)\n",
    "\n",
    "5. **Saturation**:\n",
    "   - For very large positive or negative inputs, the function saturates\n",
    "   - This can lead to the vanishing gradient problem in deep networks\n",
    "\n",
    "### Examples:\n",
    "1. **Input values and corresponding outputs**:\n",
    "   - σ(-5) ≈ 0.0067 (very close to 0)\n",
    "   - σ(-2) ≈ 0.1192\n",
    "   - σ(-1) ≈ 0.2689\n",
    "   - σ(0) = 0.5 (exactly)\n",
    "   - σ(1) ≈ 0.7311\n",
    "   - σ(2) ≈ 0.8808\n",
    "   - σ(5) ≈ 0.9933 (very close to 1)\n",
    "\n",
    "2. **Derivative examples**:\n",
    "   - At x = 0: σ'(0) = σ(0) * (1 - σ(0)) = 0.5 * 0.5 = 0.25 (maximum value)\n",
    "   - At x = 2: σ'(2) ≈ 0.8808 * (1 - 0.8808) ≈ 0.1050\n",
    "   - At x = 5: σ'(5) ≈ 0.9933 * (1 - 0.9933) ≈ 0.0067 (very small, illustrating saturation)\n",
    "\n",
    "3. **Practical example in binary classification**:\n",
    "   - Input: x = 2.5 (high positive value)\n",
    "   - Output: σ(2.5) ≈ 0.9241\n",
    "   - Interpretation: 92.41% confidence in positive class\n",
    "\n",
    "4. **Symmetry example**:\n",
    "   - σ(1) ≈ 0.7311\n",
    "   - σ(-1) ≈ 0.2689\n",
    "   - Note: 0.7311 + 0.2689 = 1, demonstrating symmetry around 0.5\n",
    "\n",
    "\n",
    "### Detailed Proof of the Sigmoid Function Derivative with Explanations\n",
    "\n",
    "We aim to prove that the derivative of the sigmoid function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$.\n",
    "\n",
    "1. Begin with the sigmoid function:\n",
    "   $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "   \n",
    "   Explanation: This is our starting point, the standard definition of the sigmoid function. It's crucial to clearly state the function we're differentiating.\n",
    "\n",
    "2. We'll use the quotient rule to find the derivative. The quotient rule states:\n",
    "   For $f(x) = \\frac{u(x)}{v(x)}$, the derivative is:\n",
    "   $$f'(x) = \\frac{u'(x)v(x) - u(x)v'(x)}{[v(x)]^2}$$\n",
    "   \n",
    "   Explanation: We choose the quotient rule because the sigmoid function is a fraction. This rule allows us to differentiate complex fractions by breaking them down into simpler parts.\n",
    "\n",
    "3. In our case:\n",
    "   $u(x) = 1$ (constant function)\n",
    "   $v(x) = 1 + e^{-x}$\n",
    "   \n",
    "   Explanation: We split the sigmoid function into numerator and denominator to apply the quotient rule. This decomposition simplifies our calculation process.\n",
    "\n",
    "4. Calculate the derivatives of $u(x)$ and $v(x)$:\n",
    "   $u'(x) = 0$ (derivative of a constant is 0)\n",
    "   $v'(x) = -e^{-x}$ (using the chain rule)\n",
    "   \n",
    "   Explanation: We find the derivatives of the numerator and denominator separately. The numerator's derivative is straightforward as it's a constant. For the denominator, we apply the chain rule, recognizing that the derivative of $e^{-x}$ is $-e^{-x}$.\n",
    "\n",
    "5. Apply the quotient rule:\n",
    "   $$\\begin{align*}\n",
    "   \\sigma'(x) &= \\frac{u'(x)v(x) - u(x)v'(x)}{[v(x)]^2} \\\\[2ex]\n",
    "   &= \\frac{0 \\cdot (1 + e^{-x}) - 1 \\cdot (-e^{-x})}{(1 + e^{-x})^2} \\\\[2ex]\n",
    "   &= \\frac{e^{-x}}{(1 + e^{-x})^2}\n",
    "   \\end{align*}$$\n",
    "   \n",
    "   Explanation: We substitute the values into the quotient rule formula. The first term becomes zero due to $u'(x) = 0$, simplifying our expression. This step is crucial as it leads us to a more manageable form of the derivative.\n",
    "\n",
    "6. Manipulate the expression:\n",
    "   $$\\begin{align*}\n",
    "   \\sigma'(x) &= \\frac{e^{-x}}{(1 + e^{-x})^2} \\\\[2ex]\n",
    "   &= \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} \\\\[2ex]\n",
    "   &= \\sigma(x) \\cdot \\frac{e^{-x}}{1 + e^{-x}} \\\\[2ex]\n",
    "   &= \\sigma(x) \\cdot (1 - \\frac{1}{1 + e^{-x}}) \\\\[2ex]\n",
    "   &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "   \\end{align*}$$\n",
    "   \n",
    "   Explanation: This final manipulation is key to our proof. We first split the fraction, recognizing that $\\frac{1}{1 + e^{-x}}$ is the original sigmoid function. We then cleverly rewrite $\\frac{e^{-x}}{1 + e^{-x}}$ as $1 - \\frac{1}{1 + e^{-x}}$, which allows us to express the entire derivative in terms of the sigmoid function itself. This step showcases the elegant relationship between the sigmoid function and its derivative.\n",
    "\n",
    "Thus, we have rigorously proven that $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$. This result is significant as it demonstrates that the derivative of the sigmoid function can be expressed entirely in terms of the function itself, a property that makes it computationally efficient and contributes to its widespread use in machine learning algorithms, particularly in the context of neural networks and logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Tanh (Hyperbolic Tangent) Activation**\n",
    "- **Equation**: \n",
    "  $$\n",
    "  \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "  $$\n",
    "- **Range**: (-1, 1)\n",
    "- **Usage**: Hidden layers in simple neural networks.\n",
    "- **Characteristics**: \n",
    "  - Similar to sigmoid but outputs values between -1 and 1, allowing stronger gradients.\n",
    "  - **Issues**: Still susceptible to the vanishing gradient problem.\n",
    "- **Use Case**: Often used in hidden layers before ReLU became common.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **ReLU (Rectified Linear Unit)**\n",
    "\n",
    "### Definition\n",
    "The Rectified Linear Unit (ReLU) is defined as:\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x) = \n",
    "\\begin{cases} \n",
    "0 & \\text{for } x < 0 \\\\\n",
    "x & \\text{for } x \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Properties\n",
    "1. **Non-linearity**: Despite its piecewise linear form, ReLU introduces non-linearity into neural networks, allowing them to learn complex patterns.\n",
    "2. **Sparsity**: ReLU naturally creates sparse representations by setting negative inputs to zero.\n",
    "3. **Range**: $[0, \\infty)$\n",
    "4. **Non-differentiable at x = 0**: The function has a \"kink\" at x = 0, making it non-differentiable at this point.\n",
    "\n",
    "### Advantages\n",
    "1. **Computational Efficiency**: ReLU is simple to compute, requiring only a max operation.\n",
    "2. **Gradient Propagation**: For positive inputs, the gradient is always 1, helping to mitigate the vanishing gradient problem.\n",
    "3. **Sparse Activation**: By outputting 0 for negative inputs, ReLU can lead to sparse representations, which can be beneficial for some tasks.\n",
    "\n",
    "### Disadvantages\n",
    "1. **Dead Neurons**: Neurons can \"die\" if they consistently output 0, leading to parts of the network becoming inactive.\n",
    "2. **Unbounded Output**: For large positive inputs, ReLU can produce arbitrarily large values, potentially leading to numerical instability.\n",
    "\n",
    "### Formal Proof of Derivative\n",
    "We will prove that the derivative of ReLU is:\n",
    "\n",
    "$$\n",
    "f'(x) = \n",
    "\\begin{cases} \n",
    "0 & \\text{for } x < 0 \\\\\n",
    "1 & \\text{for } x > 0 \\\\\n",
    "\\text{undefined} & \\text{for } x = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "1) For $x < 0$:\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   f'(x) &= \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h} \\\\\n",
    "   &= \\lim_{h \\to 0} \\frac{\\max(0, x+h) - \\max(0, x)}{h} \\\\\n",
    "   &= \\lim_{h \\to 0} \\frac{0 - 0}{h} = 0\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "2) For $x > 0$:\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   f'(x) &= \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h} \\\\\n",
    "   &= \\lim_{h \\to 0} \\frac{\\max(0, x+h) - \\max(0, x)}{h} \\\\\n",
    "   &= \\lim_{h \\to 0} \\frac{(x+h) - x}{h} = 1\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "3) For $x = 0$:\n",
    "   The left-hand and right-hand limits differ:\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   \\lim_{h \\to 0^-} \\frac{f(0+h) - f(0)}{h} &= \\lim_{h \\to 0^-} \\frac{0 - 0}{h} = 0 \\\\\n",
    "   \\lim_{h \\to 0^+} \\frac{f(0+h) - f(0)}{h} &= \\lim_{h \\to 0^+} \\frac{h - 0}{h} = 1\n",
    "   \\end{align*}\n",
    "   $$\n",
    "   Therefore, the derivative is undefined at x = 0.\n",
    "\n",
    "### Practical Considerations\n",
    "In practice, the derivative at x = 0 is often defined as either 0 or 1 for computational purposes. Some implementations use a \"leaky ReLU\" to address the dead neuron problem:\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "\\alpha x & \\text{for } x < 0 \\\\\n",
    "x & \\text{for } x \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a small positive constant (e.g., 0.01).\n",
    "\n",
    "### Use in Deep Learning\n",
    "ReLU is widely used in hidden layers of deep neural networks due to its simplicity and effectiveness in addressing the vanishing gradient problem. It has largely replaced earlier activation functions like sigmoid and tanh in many applications, especially in deep convolutional neural networks for image processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Leaky ReLU**\n",
    "- **Equation**: \n",
    "  $$\n",
    "  f(x) = x \\, \\text{for} \\, x > 0, \\, \\alpha x \\, \\text{for} \\, x \\leq 0\n",
    "  $$\n",
    "  where $\\alpha$ is a small positive constant (e.g., 0.01).\n",
    "- **Range**: (-∞, ∞)\n",
    "- **Usage**: Variant of ReLU to address dead neuron problems.\n",
    "- **Characteristics**:\n",
    "  - Allows small non-zero gradients for negative values.\n",
    "  - Prevents neurons from becoming inactive (dead).\n",
    "- **Use Case**: Used to prevent dead neurons in deeper networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Softmax Activation Function**\n",
    "\n",
    "### Definition\n",
    "The Softmax function is defined as:\n",
    "\n",
    "$$\n",
    "f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}}\n",
    "$$\n",
    "\n",
    "where $x_i$ is the input to the softmax for class $i$, and $K$ is the number of classes.\n",
    "\n",
    "### Properties\n",
    "1. **Normalization**: Softmax outputs sum to 1, creating a probability distribution.\n",
    "2. **Range**: (0, 1) for each output.\n",
    "3. **Differentiable**: Smooth and differentiable everywhere.\n",
    "4. **Monotonicity**: Preserves the order of input values.\n",
    "\n",
    "### Advantages\n",
    "1. **Probability Interpretation**: Outputs can be interpreted as probabilities.\n",
    "2. **Multi-class Classification**: Ideal for problems with mutually exclusive classes.\n",
    "3. **Differentiable**: Allows for effective gradient-based learning.\n",
    "\n",
    "### Disadvantages\n",
    "1. **Computational Cost**: Exponential operations can be expensive.\n",
    "2. **Numerical Stability**: Can suffer from overflow/underflow for large inputs.\n",
    "\n",
    "### Formal Proof of Derivative\n",
    "We will prove that the derivative of Softmax is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x_i)}{\\partial x_j} = \n",
    "\\begin{cases}\n",
    "f(x_i)(1 - f(x_i)) & \\text{if } i = j \\\\\n",
    "-f(x_i)f(x_j) & \\text{if } i \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Let $S = \\sum_{k=1}^K e^{x_k}$\n",
    "\n",
    "1) For $i = j$:\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   \\frac{\\partial f(x_i)}{\\partial x_i} &= \\frac{\\partial}{\\partial x_i} \\left(\\frac{e^{x_i}}{S}\\right) \\\\\n",
    "   &= \\frac{e^{x_i} \\cdot S - e^{x_i} \\cdot e^{x_i}}{S^2} \\\\\n",
    "   &= \\frac{e^{x_i}}{S} - \\left(\\frac{e^{x_i}}{S}\\right)^2 \\\\\n",
    "   &= f(x_i) - (f(x_i))^2 \\\\\n",
    "   &= f(x_i)(1 - f(x_i))\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "2) For $i \\neq j$:\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   \\frac{\\partial f(x_i)}{\\partial x_j} &= \\frac{\\partial}{\\partial x_j} \\left(\\frac{e^{x_i}}{S}\\right) \\\\\n",
    "   &= -\\frac{e^{x_i} \\cdot e^{x_j}}{S^2} \\\\\n",
    "   &= -\\frac{e^{x_i}}{S} \\cdot \\frac{e^{x_j}}{S} \\\\\n",
    "   &= -f(x_i)f(x_j)\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "### Practical Considerations\n",
    "1. **Numerical Stability**: To prevent overflow, it's common to subtract the maximum value from all inputs:\n",
    "   $$\n",
    "   f(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_{j=1}^K e^{x_j - \\max(x)}}\n",
    "   $$\n",
    "\n",
    "2. **Cross-Entropy Loss**: Softmax is often used with cross-entropy loss for classification tasks:\n",
    "   $$\n",
    "   L = -\\sum_{i=1}^K y_i \\log(f(x_i))\n",
    "   $$\n",
    "   where $y_i$ is the true label (0 or 1) for class $i$.\n",
    "\n",
    "### Use in Deep Learning\n",
    "Softmax is primarily used in the output layer of neural networks for multi-class classification problems. It's particularly effective when classes are mutually exclusive, such as in image classification or natural language processing tasks like sentiment analysis or language identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. **ELU (Exponential Linear Unit)**\n",
    "- **Equation**: \n",
    "  $$\n",
    "  f(x) = \n",
    "  \\begin{cases} \n",
    "    x & \\text{if } x > 0 \\\\\n",
    "    \\alpha(e^x - 1) & \\text{if } x \\leq 0 \n",
    "  \\end{cases}\n",
    "  $$\n",
    "- **Range**: (-α, ∞)\n",
    "- **Usage**: Alternative to ReLU to avoid dead neurons.\n",
    "- **Characteristics**:\n",
    "  - Reduces bias shift toward positive values.\n",
    "  - Avoids the dead neuron problem.\n",
    "  - **Issues**: More computationally expensive due to the exponential operation.\n",
    "- **Use Case**: Used in deep networks to achieve robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. **Swish**\n",
    "- **Equation**: \n",
    "  $$\n",
    "  f(x) = x \\cdot \\sigma(x)\n",
    "  $$\n",
    "  where $\\sigma(x)$ is the sigmoid function.\n",
    "- **Range**: (-∞, ∞)\n",
    "- **Usage**: A newer activation function shown to perform better in some deep learning models.\n",
    "- **Characteristics**:\n",
    "  - Allows for smooth non-linearity and better gradient flow compared to ReLU.\n",
    "  - Found to outperform ReLU in some cases.\n",
    "- **Use Case**: Used in advanced architectures such as image classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. **ReLU6**\n",
    "- **Equation**: \n",
    "  $$\n",
    "  f(x) = \\min(\\max(0, x), 6)\n",
    "  $$\n",
    "- **Range**: [0, 6]\n",
    "- **Usage**: Common in mobile-friendly models like MobileNet.\n",
    "- **Characteristics**:\n",
    "  - Capped version of ReLU to help with overflow issues.\n",
    "  - Improves model robustness.\n",
    "- **Use Case**: Used in mobile and edge-device models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table of Activation Functions\n",
    "\n",
    "| **Activation Function** | **Equation**                               | **Output Range**  | **Common Usage**              |\n",
    "|------------------------|--------------------------------------------|------------------|-------------------------------|\n",
    "| **Sigmoid**             | $\\sigma(x) = \\frac{1}{1 + e^{-x}}$         | (0, 1)           | Binary classification         |\n",
    "| **Tanh**                | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1)          | Hidden layers in neural nets  |\n",
    "| **ReLU**                | $f(x) = \\max(0, x)$                        | [0, ∞)           | Deep neural networks          |\n",
    "| **Leaky ReLU**          | $f(x) = \\alpha x \\, \\text{if} \\, x < 0 \\, \\text{else} \\, x$ | (-∞, ∞) | Avoiding dead neurons         |\n",
    "| **Softmax**             | $f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$  | (0, 1)           | Multi-class classification    |\n",
    "| **ELU**                 | $f(x) = \\alpha(e^x - 1) \\, \\text{if} \\, x \\leq 0, \\, x \\, \\text{otherwise}$ | (-α, ∞) | Deep networks for robust learning |\n",
    "| **Swish**               | $f(x) = x \\cdot \\sigma(x)$                 | (-∞, ∞)          | Advanced architectures        |\n",
    "| **ReLU6**               | $f(x) = \\min(\\max(0, x), 6)$               | [0, 6]           | Mobile and embedded models    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final equation of the multilayer network is:\n",
    "\n",
    "$$\n",
    "y = \\text{softmax}(W_3 \\cdot \\sigma(W_2 \\cdot \\sigma(W_1 \\cdot \\max(0, W_0x + b_0) + b_1) + b_2) + b_3)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y$ is the output vector\n",
    "- $x$ is the input vector\n",
    "- $W_i$ and $b_i$ are the weights and biases for layer $i$\n",
    "- $\\max(0, z)$ is the ReLU function\n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the Sigmoid function\n",
    "- $\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$ is the Softmax function\n",
    "\n",
    "This equation represents the forward pass through the entire network, from input to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of the Final Equation for the Multilayer Network\n",
    "\n",
    "Let's derive the final equation step by step, starting from the input and moving through each layer.\n",
    "\n",
    "Given:\n",
    "- Input vector: $x$\n",
    "- Weights and biases for each layer: $W_i$ and $b_i$\n",
    "- ReLU activation: $f_{\\text{ReLU}}(z) = \\max(0, z)$\n",
    "- Sigmoid activation: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- Softmax activation: $\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$\n",
    "\n",
    "### Step 1: Layer 0 (ReLU)\n",
    "The output of the first layer is:\n",
    "$$a_0 = f_{\\text{ReLU}}(W_0x + b_0) = \\max(0, W_0x + b_0)$$\n",
    "\n",
    "### Step 2: Layer 1 (Sigmoid)\n",
    "The output of the second layer is:\n",
    "$$a_1 = \\sigma(W_1a_0 + b_1)$$\n",
    "\n",
    "Substituting $a_0$ from Step 1:\n",
    "$$a_1 = \\sigma(W_1 \\cdot \\max(0, W_0x + b_0) + b_1)$$\n",
    "\n",
    "### Step 3: Layer 2 (Sigmoid)\n",
    "The output of the third layer is:\n",
    "$$a_2 = \\sigma(W_2a_1 + b_2)$$\n",
    "\n",
    "Substituting $a_1$ from Step 2:\n",
    "$$a_2 = \\sigma(W_2 \\cdot \\sigma(W_1 \\cdot \\max(0, W_0x + b_0) + b_1) + b_2)$$\n",
    "\n",
    "### Step 4: Layer 3 (Softmax)\n",
    "The final output of the network is:\n",
    "$$y = \\text{softmax}(W_3a_2 + b_3)$$\n",
    "\n",
    "Substituting $a_2$ from Step 3:\n",
    "$$y = \\text{softmax}(W_3 \\cdot \\sigma(W_2 \\cdot \\sigma(W_1 \\cdot \\max(0, W_0x + b_0) + b_1) + b_2) + b_3)$$\n",
    "\n",
    "This is our final equation, representing the complete forward pass through the network.\n",
    "\n",
    "### Conclusion\n",
    "We have derived the final equation by composing the functions for each layer, starting from the input and moving through each activation. The nested structure of the equation reflects the layered architecture of the neural network, with each activation function applied in sequence."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
